{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719511b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.66-py2.py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (2.32.3)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Downloading multitasking-0.0.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (4.3.6)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (2024.1)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Downloading frozendict-2.4.7-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Downloading peewee-3.18.3.tar.gz (3.0 MB)\n",
      "     ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
      "     ------------- -------------------------- 1.0/3.0 MB 2.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.6/3.0 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 2.4/3.0 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.0/3.0 MB 3.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (4.12.3)\n",
      "Collecting curl_cffi>=0.7 (from yfinance)\n",
      "  Downloading curl_cffi-0.13.0-cp39-abi3-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (6.33.2)\n",
      "Collecting websockets>=13.0 (from yfinance)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (2.2.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n",
      "Downloading yfinance-0.2.66-py2.py3-none-any.whl (123 kB)\n",
      "Downloading curl_cffi-0.13.0-cp39-abi3-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 1.3/1.6 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 6.2 MB/s eta 0:00:00\n",
      "Downloading frozendict-2.4.7-py3-none-any.whl (16 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Building wheels for collected packages: multitasking, peewee\n",
      "  Building wheel for multitasking (setup.py): started\n",
      "  Building wheel for multitasking (setup.py): finished with status 'done'\n",
      "  Created wheel for multitasking: filename=multitasking-0.0.12-py3-none-any.whl size=15617 sha256=800d69fe8af2a5a50babba15f702ede0aa26a13dcb0904a8ad4fa15ac3651f2a\n",
      "  Stored in directory: c:\\users\\sivapriya\\appdata\\local\\pip\\cache\\wheels\\cc\\bd\\6f\\664d62c99327abeef7d86489e6631cbf45b56fbf7ef1d6ef00\n",
      "  Building wheel for peewee (pyproject.toml): started\n",
      "  Building wheel for peewee (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for peewee: filename=peewee-3.18.3-py3-none-any.whl size=139181 sha256=8cc9c0f6fa99e16d1acb8a38403bada1bdc9a4fd102405b93c6bce10f299c2a6\n",
      "  Stored in directory: c:\\users\\sivapriya\\appdata\\local\\pip\\cache\\wheels\\e2\\48\\b6\\675a31c56e50b8b343e1ffbb1d9209f0d95025e2cfa0bbeeed\n",
      "Successfully built multitasking peewee\n",
      "Installing collected packages: peewee, multitasking, websockets, frozendict, curl_cffi, yfinance\n",
      "Successfully installed curl_cffi-0.13.0 frozendict-2.4.7 multitasking-0.0.12 peewee-3.18.3 websockets-15.0.1 yfinance-0.2.66\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "  WARNING: The script websockets.exe is installed in 'c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script sample.exe is installed in 'c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c91c4ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers: 600\n",
      "Date range: 2009-04-08 → 2023-12-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*                      3%                       ]  17 of 600 completedHTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: BHLB\"}}}\n",
      "[*********************100%***********************]  600 of 600 completed\n",
      "\n",
      "61 Failed downloads:\n",
      "['BHLB', 'SASR', 'X', 'ATSG', 'SOLO', 'INFN', 'COOP', 'YY', 'ACRX', 'CARA', 'CEI', 'DLA', 'MORF', 'PZC', 'SAVE', 'CPE', 'APDN', 'PMF', 'ENZ', 'PMX', 'CBAY', 'CEIX', 'YNDX', 'AINC', 'MRNS', 'GOL', 'ADES', 'SOI', 'PRMW', 'EVBN', 'SWI', 'TGH', 'NVTA', 'SLCA', 'ESGR', 'HARP', 'FIF', 'MRTX', 'DADA', 'CDMO', 'PEAK', 'EBIX', 'SP', 'FLIC', 'FUV', 'PFC', 'ARCH', 'SPTN', 'NEPT', 'TEDU', 'CMRX', 'PHT', 'INFI', 'AE']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "['TWOU', 'SPI', 'AFMD', 'CSSE']: YFPricesMissingError('possibly delisted; no price data found  (1d 2009-04-08 -> 2023-12-16)')\n",
      "['NRGU', 'VRM']: YFPricesMissingError('possibly delisted; no price data found  (1d 2009-04-08 -> 2023-12-16) (Yahoo error = \"Data doesn\\'t exist for startDate = 1239163200, endDate = 1702702800\")')\n",
      "['FNHC']: YFPricesMissingError('possibly delisted; no price data found  (1d 2009-04-08 -> 2023-12-16) (Yahoo error = \"No data found, symbol may be delisted\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Price data saved for all available tickers.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load dataset\n",
    "# ------------------------------------------------------------\n",
    "df = pd.read_parquet(\"sampled_35k_embedded.parquet\")\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "tickers = sorted(df[\"Stock_symbol\"].dropna().unique().tolist())\n",
    "start_date = df[\"Date\"].min().strftime(\"%Y-%m-%d\")\n",
    "end_date = df[\"Date\"].max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Tickers: {len(tickers)}\")\n",
    "print(f\"Date range: {start_date} → {end_date}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Download prices\n",
    "# ------------------------------------------------------------\n",
    "tickers_str = \" \".join(tickers)\n",
    "\n",
    "prices = yf.download(\n",
    "    tickers_str,\n",
    "    start=start_date,\n",
    "    end=end_date,\n",
    "    auto_adjust=True,\n",
    "    group_by=\"ticker\",\n",
    "    threads=True,\n",
    "    progress=True\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save per-ticker parquet files\n",
    "# ------------------------------------------------------------\n",
    "os.makedirs(\"prices_chunks\", exist_ok=True)\n",
    "\n",
    "for t in tickers:\n",
    "    if t not in prices:\n",
    "        continue\n",
    "\n",
    "    df_t = prices[t].dropna(how=\"all\")\n",
    "    if df_t.empty:\n",
    "        continue\n",
    "\n",
    "    df_t[\"Stock_symbol\"] = t\n",
    "    df_t.reset_index(inplace=True)\n",
    "    df_t.to_parquet(f\"prices_chunks/{t}.parquet\")\n",
    "\n",
    "print(\"✅ Price data saved for all available tickers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "919b2aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_prices.parquet written successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "\n",
    "def normalize_prices(df):\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [c[0] for c in df.columns]\n",
    "\n",
    "    keep = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Stock_symbol\"]\n",
    "    df = df[keep]\n",
    "\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "    df[[\"Open\", \"High\", \"Low\", \"Close\"]] = df[\n",
    "        [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "    ].astype(\"float32\")\n",
    "\n",
    "    df[\"Volume\"] = df[\"Volume\"].astype(\"int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "price_files = glob.glob(\"prices_chunks/*.parquet\")\n",
    "\n",
    "writer = None\n",
    "\n",
    "for f in price_files:\n",
    "    df = pd.read_parquet(f)\n",
    "    df = normalize_prices(df)\n",
    "\n",
    "    table = pa.Table.from_pandas(\n",
    "        df,\n",
    "        preserve_index=False,\n",
    "        nthreads=1 \n",
    "    )\n",
    "\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(\n",
    "            \"all_prices.parquet\",\n",
    "            table.schema,\n",
    "            compression=\"snappy\"\n",
    "        )\n",
    "\n",
    "    writer.write_table(table)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(\"all_prices.parquet written successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20cc898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Stock_symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-04-08</td>\n",
       "      <td>34.503216</td>\n",
       "      <td>35.364933</td>\n",
       "      <td>33.874161</td>\n",
       "      <td>35.054714</td>\n",
       "      <td>1872200</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-04-09</td>\n",
       "      <td>35.606201</td>\n",
       "      <td>36.623032</td>\n",
       "      <td>35.606201</td>\n",
       "      <td>36.442070</td>\n",
       "      <td>2094600</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-04-13</td>\n",
       "      <td>36.235264</td>\n",
       "      <td>36.321438</td>\n",
       "      <td>34.951305</td>\n",
       "      <td>35.520039</td>\n",
       "      <td>1845400</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-04-14</td>\n",
       "      <td>35.235676</td>\n",
       "      <td>35.425255</td>\n",
       "      <td>34.218845</td>\n",
       "      <td>35.003010</td>\n",
       "      <td>1608900</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-04-15</td>\n",
       "      <td>34.899593</td>\n",
       "      <td>36.330048</td>\n",
       "      <td>34.546288</td>\n",
       "      <td>36.252491</td>\n",
       "      <td>2367600</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date       Open       High        Low      Close   Volume Stock_symbol\n",
       "0 2009-04-08  34.503216  35.364933  33.874161  35.054714  1872200          AAP\n",
       "1 2009-04-09  35.606201  36.623032  35.606201  36.442070  2094600          AAP\n",
       "2 2009-04-13  36.235264  36.321438  34.951305  35.520039  1845400          AAP\n",
       "3 2009-04-14  35.235676  35.425255  34.218845  35.003010  1608900          AAP\n",
       "4 2009-04-15  34.899593  36.330048  34.546288  36.252491  2367600          AAP"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "prices_df = pd.read_parquet(\"all_prices.parquet\")\n",
    "\n",
    "prices_df['Date'] = pd.to_datetime(prices_df['Date'])\n",
    "prices_df = prices_df.sort_values(['Stock_symbol', 'Date'])\n",
    "\n",
    "prices_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33645596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log returns\n",
    "prices_df['Return'] = (\n",
    "    np.log(prices_df['Close'])\n",
    "    - np.log(prices_df.groupby('Stock_symbol')['Close'].shift(1))\n",
    ")\n",
    "\n",
    "# Next-day return (THIS is what SSD predicts)\n",
    "prices_df['Return_t+1'] = (\n",
    "    prices_df.groupby('Stock_symbol')['Return'].shift(-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d14114",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df['Volatility'] = (\n",
    "    prices_df\n",
    "    .groupby('Stock_symbol')['Return']\n",
    "    .rolling(window=5)\n",
    "    .std()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50ec34ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "market_data.parquet saved\n"
     ]
    }
   ],
   "source": [
    "market_df = prices_df[\n",
    "    ['Date', 'Stock_symbol', 'Return', 'Return_t+1', 'Volatility', 'Volume']\n",
    "].copy()\n",
    "\n",
    "market_df = market_df.dropna()\n",
    "\n",
    "market_df.to_parquet(\"market_data.parquet\")\n",
    "print(\"market_data.parquet saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf6e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "# Open SSD parquet as a PyArrow dataset\n",
    "dataset = ds.dataset(\"ssd_final_variants.parquet\", format=\"parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2856fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats.mstats import winsorize\n",
    "from linearmodels.panel import PanelOLS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e412b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by stock & date\n",
    "df = df.sort_values(['Stock_symbol', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Clean volatility\n",
    "df['Vol_clean'] = winsorize(df['Volatility'], limits=[0.01, 0.01])\n",
    "\n",
    "# Lags\n",
    "for lag in range(1, 6):\n",
    "    df[f'Lag_Vol_{lag}'] = df.groupby('Stock_symbol')['Vol_clean'].shift(lag)\n",
    "\n",
    "df['Lag_Vol_mean5'] = df[[f'Lag_Vol_{i}' for i in range(1, 6)]].mean(axis=1)\n",
    "\n",
    "# Impulse response / predictive\n",
    "df['Vol_t'] = df['Vol_clean']\n",
    "df['Vol_t_plus_1'] = df.groupby('Stock_symbol')['Vol_clean'].shift(-1)\n",
    "df['Vol_t_minus_2'] = df.groupby('Stock_symbol')['Vol_clean'].shift(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37387214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "  WARNING: The script dask.exe is installed in 'c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask[dataframe]\n",
      "  Downloading dask-2025.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[dataframe]) (8.1.7)\n",
      "Collecting cloudpickle>=3.0.0 (from dask[dataframe])\n",
      "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[dataframe]) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[dataframe]) (24.0)\n",
      "Collecting partd>=1.4.0 (from dask[dataframe])\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[dataframe]) (6.0.2)\n",
      "Collecting toolz>=0.12.0 (from dask[dataframe])\n",
      "  Downloading toolz-1.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pandas>=2.0 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[dataframe]) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[dataframe]) (19.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click>=8.1->dask[dataframe]) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0->dask[dataframe]) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0->dask[dataframe]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0->dask[dataframe]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0->dask[dataframe]) (2024.1)\n",
      "Collecting locket (from partd>=1.4.0->dask[dataframe])\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[dataframe]) (1.16.0)\n",
      "Downloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading toolz-1.1.0-py3-none-any.whl (58 kB)\n",
      "Downloading dask-2025.12.0-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 932.9 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.5/1.5 MB 932.9 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 0.8/1.5 MB 714.3 kB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.8/1.5 MB 714.3 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 671.0 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 671.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 657.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 657.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 624.2 kB/s eta 0:00:00\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: toolz, locket, cloudpickle, partd, dask\n",
      "Successfully installed cloudpickle-3.1.2 dask-2025.12.0 locket-1.0.0 partd-1.4.2 toolz-1.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install dask[dataframe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34bb682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing SSD metric: SSD_cosine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\function_base.py:4824: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  arr.partition(\n",
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linearmodels\\panel\\model.py:1258: MissingValueWarning: \n",
      "Inputs contain missing values. Dropping rows with missing observations.\n",
      "  super().__init__(dependent, exog, weights=weights, check_rank=check_rank)\n",
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linearmodels\\panel\\model.py:1258: MissingValueWarning: \n",
      "Inputs contain missing values. Dropping rows with missing observations.\n",
      "  super().__init__(dependent, exog, weights=weights, check_rank=check_rank)\n",
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linearmodels\\panel\\model.py:1258: MissingValueWarning: \n",
      "Inputs contain missing values. Dropping rows with missing observations.\n",
      "  super().__init__(dependent, exog, weights=weights, check_rank=check_rank)\n",
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linearmodels\\panel\\model.py:1258: MissingValueWarning: \n",
      "Inputs contain missing values. Dropping rows with missing observations.\n",
      "  super().__init__(dependent, exog, weights=weights, check_rank=check_rank)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing SSD metric: SSD_l2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\function_base.py:4824: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  arr.partition(\n",
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linearmodels\\panel\\model.py:1258: MissingValueWarning: \n",
      "Inputs contain missing values. Dropping rows with missing observations.\n",
      "  super().__init__(dependent, exog, weights=weights, check_rank=check_rank)\n",
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linearmodels\\panel\\model.py:1258: MissingValueWarning: \n",
      "Inputs contain missing values. Dropping rows with missing observations.\n",
      "  super().__init__(dependent, exog, weights=weights, check_rank=check_rank)\n",
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linearmodels\\panel\\model.py:1258: MissingValueWarning: \n",
      "Inputs contain missing values. Dropping rows with missing observations.\n",
      "  super().__init__(dependent, exog, weights=weights, check_rank=check_rank)\n",
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linearmodels\\panel\\model.py:1258: MissingValueWarning: \n",
      "Inputs contain missing values. Dropping rows with missing observations.\n",
      "  super().__init__(dependent, exog, weights=weights, check_rank=check_rank)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing SSD metric: SSD_angular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\function_base.py:4824: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  arr.partition(\n",
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linearmodels\\panel\\model.py:1258: MissingValueWarning: \n",
      "Inputs contain missing values. Dropping rows with missing observations.\n",
      "  super().__init__(dependent, exog, weights=weights, check_rank=check_rank)\n",
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linearmodels\\panel\\model.py:1258: MissingValueWarning: \n",
      "Inputs contain missing values. Dropping rows with missing observations.\n",
      "  super().__init__(dependent, exog, weights=weights, check_rank=check_rank)\n",
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linearmodels\\panel\\model.py:1258: MissingValueWarning: \n",
      "Inputs contain missing values. Dropping rows with missing observations.\n",
      "  super().__init__(dependent, exog, weights=weights, check_rank=check_rank)\n",
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\linearmodels\\panel\\model.py:1258: MissingValueWarning: \n",
      "Inputs contain missing values. Dropping rows with missing observations.\n",
      "  super().__init__(dependent, exog, weights=weights, check_rank=check_rank)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All SSD metrics results:\n",
      "    SSD_metric  Placebo_coef  Contemp_coef  Predictive_coef  NonLinear_coef  \\\n",
      "0   SSD_cosine     -0.000849     -0.005418        -0.005446        0.003954   \n",
      "1       SSD_l2     -0.000741     -0.004650        -0.004768        0.002353   \n",
      "2  SSD_angular     -0.001772     -0.011716        -0.011950        0.006152   \n",
      "\n",
      "   Predictive_SD_impact  \n",
      "0             -0.000743  \n",
      "1             -0.000767  \n",
      "2             -0.000780  \n",
      "\n",
      "✅ Results saved to:\n",
      "D:\\ssd_panel_results.csv\n",
      "D:\\ssd_panel_results.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats.mstats import winsorize\n",
    "from linearmodels.panel import PanelOLS\n",
    "\n",
    "# ----------------------------\n",
    "# 1️⃣ Load market data\n",
    "# ----------------------------\n",
    "market_df = pd.read_parquet(\"market_data.parquet\")\n",
    "market_df['Date'] = pd.to_datetime(market_df['Date'])\n",
    "\n",
    "# ----------------------------\n",
    "# 2️⃣ Load SSD variants\n",
    "# ----------------------------\n",
    "ssd_df = pd.read_parquet(\"ssd_final_variants.parquet\")\n",
    "ssd_df['Date'] = pd.to_datetime(ssd_df['Date'])\n",
    "\n",
    "# SSD metrics to loop over\n",
    "ssd_metrics = ['SSD_cosine', 'SSD_l2', 'SSD_angular']\n",
    "\n",
    "# Store results\n",
    "all_results = []\n",
    "\n",
    "# ----------------------------\n",
    "# 3️⃣ Loop over SSD variants\n",
    "# ----------------------------\n",
    "for metric in ssd_metrics:\n",
    "    print(f\"\\nProcessing SSD metric: {metric}\")\n",
    "\n",
    "    # Keep only needed columns\n",
    "    df = ssd_df[['Stock_symbol', 'Date', 'EMA_alpha', metric]].copy()\n",
    "    df = df.rename(columns={metric: 'SSD'})\n",
    "\n",
    "    # Merge with market data\n",
    "    df = df.merge(market_df, on=['Stock_symbol', 'Date'], how='inner')\n",
    "\n",
    "    # ----------------------------\n",
    "    # Sorting & cleaning\n",
    "    # ----------------------------\n",
    "    df = df.sort_values(['Stock_symbol', 'Date']).reset_index(drop=True)\n",
    "\n",
    "    df['SSD_clean'] = winsorize(df['SSD'], limits=[0.01, 0.01])\n",
    "    df['Vol_clean'] = winsorize(df['Volatility'], limits=[0.01, 0.01])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Lagged volatility (mean of last 5 days)\n",
    "    # ----------------------------\n",
    "    for lag in range(1, 6):\n",
    "        df[f'Lag_Vol_{lag}'] = df.groupby('Stock_symbol')['Vol_clean'].shift(lag)\n",
    "\n",
    "    df['Lag_Vol_mean5'] = df[[f'Lag_Vol_{i}' for i in range(1, 6)]].mean(axis=1)\n",
    "\n",
    "    # ----------------------------\n",
    "    # News volume\n",
    "    # ----------------------------\n",
    "    news_counts = (\n",
    "        df.groupby(['Stock_symbol', 'Date'])\n",
    "          .size()\n",
    "          .reset_index(name='News_Count')\n",
    "    )\n",
    "\n",
    "    df = df.merge(news_counts, on=['Stock_symbol', 'Date'], how='left')\n",
    "    df['Log_News_Count'] = np.log1p(df['News_Count'])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Orthogonalize SSD\n",
    "    # ----------------------------\n",
    "    X_ortho = sm.add_constant(df[['Log_News_Count']])\n",
    "    y_ortho = df['SSD_clean']\n",
    "\n",
    "    mask = X_ortho.notna().all(axis=1) & y_ortho.notna()\n",
    "\n",
    "    ortho_model = sm.OLS(y_ortho[mask], X_ortho.loc[mask]).fit()\n",
    "\n",
    "    df.loc[mask, 'SSD_Pure_raw'] = ortho_model.resid\n",
    "    df['SSD_Pure'] = winsorize(df['SSD_Pure_raw'], limits=[0.01, 0.01])\n",
    "\n",
    "    # ----------------------------\n",
    "    # High-shock construction\n",
    "    # ----------------------------\n",
    "    shock_cut = df['SSD_clean'].quantile(0.90)\n",
    "    df['High_Shock'] = (df['SSD_clean'] >= shock_cut).astype(int)\n",
    "\n",
    "    df['SSD_Pure_centered'] = df['SSD_Pure'] - df['SSD_Pure'].mean()\n",
    "    df['SSD_HighShock'] = df['SSD_Pure_centered'] * df['High_Shock']\n",
    "\n",
    "    # ----------------------------\n",
    "    # Volatility leads & lags\n",
    "    # ----------------------------\n",
    "    df['Vol_t'] = df['Vol_clean']\n",
    "    df['Vol_t_plus_1'] = df.groupby('Stock_symbol')['Vol_clean'].shift(-1)\n",
    "    df['Vol_t_minus_2'] = df.groupby('Stock_symbol')['Vol_clean'].shift(2)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Panel setup\n",
    "    # ----------------------------\n",
    "    panel_df = df.set_index(['Stock_symbol', 'Date'])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Panel regressions\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        # Placebo\n",
    "        model_placebo = PanelOLS.from_formula(\n",
    "            'Vol_t_minus_2 ~ SSD_Pure_centered + Lag_Vol_mean5 + Log_News_Count + EntityEffects + TimeEffects',\n",
    "            data=panel_df\n",
    "        ).fit(cov_type='clustered', cluster_entity=True, cluster_time=True)\n",
    "\n",
    "        # Contemporaneous\n",
    "        model_contemp = PanelOLS.from_formula(\n",
    "            'Vol_t ~ SSD_Pure_centered + Lag_Vol_mean5 + Log_News_Count + EntityEffects + TimeEffects',\n",
    "            data=panel_df\n",
    "        ).fit(cov_type='clustered', cluster_entity=True, cluster_time=True)\n",
    "\n",
    "        # Predictive\n",
    "        model_predictive = PanelOLS.from_formula(\n",
    "            'Vol_t_plus_1 ~ SSD_Pure_centered + Lag_Vol_mean5 + Log_News_Count + EntityEffects + TimeEffects',\n",
    "            data=panel_df\n",
    "        ).fit(cov_type='clustered', cluster_entity=True, cluster_time=True)\n",
    "\n",
    "        # Non-linearity\n",
    "        model_nl = PanelOLS.from_formula(\n",
    "            'Vol_t_plus_1 ~ SSD_Pure_centered + SSD_HighShock + Lag_Vol_mean5 + Log_News_Count + EntityEffects + TimeEffects',\n",
    "            data=panel_df\n",
    "        ).fit(cov_type='clustered', cluster_entity=True, cluster_time=True)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Store results\n",
    "        # ----------------------------\n",
    "        ssd_std = df['SSD_Pure_centered'].std()\n",
    "\n",
    "        all_results.append({\n",
    "            'SSD_metric': metric,\n",
    "            'Placebo_coef': model_placebo.params['SSD_Pure_centered'],\n",
    "            'Contemp_coef': model_contemp.params['SSD_Pure_centered'],\n",
    "            'Predictive_coef': model_predictive.params['SSD_Pure_centered'],\n",
    "            'NonLinear_coef': model_nl.params.get('SSD_HighShock', np.nan),\n",
    "            'Predictive_SD_impact': model_predictive.params['SSD_Pure_centered'] * ssd_std\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for metric {metric}: {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4️⃣ Final results\n",
    "# ----------------------------\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\nAll SSD metrics results:\")\n",
    "print(results_df)\n",
    "\n",
    "# ----------------------------\n",
    "# 5️⃣ Save to D drive\n",
    "# ----------------------------\n",
    "output_csv = r\"D:\\ssd_panel_results.csv\"\n",
    "output_parquet = r\"D:\\ssd_panel_results.parquet\"\n",
    "\n",
    "results_df.to_csv(output_csv, index=False)\n",
    "results_df.to_parquet(output_parquet, index=False)\n",
    "\n",
    "print(f\"\\n✅ Results saved to:\")\n",
    "print(output_csv)\n",
    "print(output_parquet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
