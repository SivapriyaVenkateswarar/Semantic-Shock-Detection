{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17764d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckdb in c:\\users\\sivapriya\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e12fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import duckdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e555c25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sampled rows: 358993\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "file_path = r\"P:\\IIM Ranchi\\nasdaq_exteral_data.csv\"\n",
    "\n",
    "duckdb_conn = duckdb.connect()\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH raw AS (\n",
    "    SELECT *\n",
    "    FROM read_csv(\n",
    "        'P:/IIM Ranchi/nasdaq_exteral_data.csv',\n",
    "        AUTO_DETECT=TRUE,\n",
    "        IGNORE_ERRORS=TRUE\n",
    "    )\n",
    "    WHERE Article IS NOT NULL AND TRIM(Article) <> ''\n",
    "),\n",
    "daily_stats AS (\n",
    "    SELECT \n",
    "        Stock_symbol, \n",
    "        Date, \n",
    "        COUNT(*) as articles_per_day\n",
    "    FROM raw\n",
    "    GROUP BY 1, 2\n",
    "),\n",
    "qualified_stocks AS (\n",
    "    SELECT \n",
    "        Stock_symbol,\n",
    "        COUNT(DISTINCT Date) as active_days,\n",
    "        AVG(articles_per_day) as avg_density\n",
    "    FROM daily_stats\n",
    "    GROUP BY 1\n",
    "    HAVING active_days > 50 \n",
    "       AND avg_density BETWEEN 1 AND 15\n",
    "),\n",
    "sampled_firms AS (\n",
    "    SELECT Stock_symbol \n",
    "    FROM qualified_stocks\n",
    "    USING SAMPLE reservoir(600)\n",
    ")\n",
    "SELECT r.*\n",
    "FROM raw r\n",
    "JOIN sampled_firms s \n",
    "ON r.Stock_symbol = s.Stock_symbol;\n",
    "\"\"\"\n",
    "\n",
    "sampled_df = duckdb_conn.execute(query).df()\n",
    "sampled_df.to_parquet(\"sampled_35k_news.parquet\")\n",
    "\n",
    "print(\"Final sampled rows:\", len(sampled_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298b03a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358993\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"sampled_35k_news.parquet\")\n",
    "\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e9d276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date Stock_symbol                                            Article\n",
      "0 2010-04-20          AAP  Altera Corp. ( ALTR ), Hudson City Bancorp Inc...\n",
      "1 2010-05-20          AAP  Among the biggest winners in Thursday's early ...\n",
      "2 2010-05-20          AAP  Auto parts retailer Advance Auto Parts, Inc. (...\n",
      "3 2010-05-20          AAP  The market continued to sell off here and we c...\n",
      "4 2010-10-10          AAP  Working for StreetAuthority, I do a lot of dif...\n",
      "Cleaned size: 358993\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"sampled_35k_news.parquet\")\n",
    "\n",
    "# Keep essential columns — adjust names as needed\n",
    "expected_cols = [\"Date\", \"Stock_symbol\", \"Article\"]\n",
    "df = df[[c for c in expected_cols if c in df.columns]]\n",
    "\n",
    "# Clean missing/empty articles\n",
    "df = df.dropna(subset=['Article'])\n",
    "df = df[df['Article'].str.strip() != \"\"]\n",
    "\n",
    "# Parse dates\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Sort for consistency\n",
    "df = df.sort_values([\"Stock_symbol\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "df.to_parquet(\"sampled_35k_clean.parquet\")\n",
    "print(df.head())\n",
    "print(\"Cleaned size:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d6d8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Sivapriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3740/3740 [5:31:09<00:00,  5.31s/it]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done embedding!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "df = pd.read_parquet(\"sampled_35k_clean.parquet\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model on GPU\n",
    "model = SentenceTransformer(\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "def embed_batch(texts, batch_size=96):  \n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        emb = model.encode(\n",
    "            batch,\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        all_embs.append(emb)\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "df[\"embedding\"] = list(embed_batch(df[\"Article\"].tolist()))\n",
    "df.to_parquet(\"sampled_35k_embedded.parquet\")\n",
    "\n",
    "print(\"Done embedding!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6957b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"None\")\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23bfbfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Daily embeddings saved: daily_embeddings.parquet\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "# ============================================================\n",
    "# 0. Config\n",
    "# ============================================================\n",
    "\n",
    "PARQUET_PATH = \"sampled_35k_embedded.parquet\"\n",
    "OUT_PATH = \"daily_embeddings.parquet\"\n",
    "\n",
    "BATCH_SIZE = 2000        # Arrow batch size (safe on Windows)\n",
    "EMB_DTYPE = np.float32  # halve memory\n",
    "\n",
    "# ============================================================\n",
    "# 1. Novelty-weighted aggregation (OOM-safe)\n",
    "# ============================================================\n",
    "\n",
    "def novelty_weighted_mean_fast(embs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    embs: (n_docs, dim) float32\n",
    "    returns: (dim,) float32, L2-normalized\n",
    "    \"\"\"\n",
    "\n",
    "    # Single document\n",
    "    if embs.shape[0] == 1:\n",
    "        v = embs[0]\n",
    "        return v / np.linalg.norm(v)\n",
    "\n",
    "    # Normalize per-document\n",
    "    embs = embs / np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "\n",
    "    # Semantic center\n",
    "    center = embs.mean(axis=0)\n",
    "    center /= np.linalg.norm(center)\n",
    "\n",
    "    # Novelty weights\n",
    "    weights = 1.0 - (embs @ center)\n",
    "    weights[weights < 0.0] = 0.0\n",
    "\n",
    "    wsum = weights.sum()\n",
    "    if wsum == 0.0:\n",
    "        return center\n",
    "\n",
    "    out = (embs * weights[:, None]).sum(axis=0) / wsum\n",
    "    return out / np.linalg.norm(out)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Stream parquet safely (NO large allocations)\n",
    "# ============================================================\n",
    "\n",
    "dataset = ds.dataset(PARQUET_PATH, format=\"parquet\")\n",
    "\n",
    "daily_groups = {}  # (stock, date) -> list of embeddings\n",
    "\n",
    "for batch in dataset.to_batches(batch_size=BATCH_SIZE):\n",
    "    df = batch.to_pandas()\n",
    "\n",
    "    # Convert embeddings safely\n",
    "    df[\"embedding\"] = df[\"embedding\"].apply(\n",
    "        lambda x: np.asarray(x, dtype=EMB_DTYPE)\n",
    "    )\n",
    "\n",
    "    # Accumulate by (Stock_symbol, Date)\n",
    "    for (stock, date), g in df.groupby([\"Stock_symbol\", \"Date\"], sort=False):\n",
    "        key = (stock, date)\n",
    "        if key not in daily_groups:\n",
    "            daily_groups[key] = []\n",
    "        daily_groups[key].extend(g[\"embedding\"].values)\n",
    "\n",
    "    # Free batch explicitly (important on Windows)\n",
    "    del df, batch\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Compute daily embeddings\n",
    "# ============================================================\n",
    "\n",
    "records = []\n",
    "\n",
    "for (stock, date), embs in daily_groups.items():\n",
    "    E_t = novelty_weighted_mean_fast(np.vstack(embs))\n",
    "    records.append((stock, date, E_t))\n",
    "\n",
    "# Cleanup\n",
    "del daily_groups\n",
    "\n",
    "# ============================================================\n",
    "# 4. Final dataframe\n",
    "# ============================================================\n",
    "\n",
    "out_df = pd.DataFrame(\n",
    "    records,\n",
    "    columns=[\"Stock_symbol\", \"Date\", \"E_t\"]\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 5. Save novelty-weighted daily embeddings\n",
    "# ============================================================\n",
    "\n",
    "out_df.to_parquet(\n",
    "    OUT_PATH,\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"zstd\"\n",
    ")\n",
    "\n",
    "print(f\"✅ Daily embeddings saved: {OUT_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae610c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SSD variants computed: (664404, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Load precomputed daily embeddings\n",
    "daily_embeddings = pd.read_parquet(\"daily_embeddings.parquet\")\n",
    "\n",
    "# ----------------------------\n",
    "# Distance functions\n",
    "# ----------------------------\n",
    "def cosine_distance(a, b):\n",
    "    return 1 - np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "def l2_distance(a, b):\n",
    "    return norm(a - b)\n",
    "\n",
    "def angular_distance(a, b):\n",
    "    cos_sim = np.dot(a, b) / (norm(a) * norm(b))\n",
    "    return np.arccos(np.clip(cos_sim, -1.0, 1.0)) / np.pi  # normalized [0,1]\n",
    "\n",
    "# ----------------------------\n",
    "# EMA function\n",
    "# ----------------------------\n",
    "def compute_ema(arr, alpha):\n",
    "    \"\"\"arr: np.array(T, dim)\"\"\"\n",
    "    ema = arr[0]\n",
    "    out = [ema]\n",
    "    for v in arr[1:]:\n",
    "        ema = alpha * v + (1 - alpha) * ema\n",
    "        out.append(ema)\n",
    "    return np.vstack(out)\n",
    "\n",
    "# ----------------------------\n",
    "# α variants\n",
    "# ----------------------------\n",
    "alphas = [0.1, 0.2, 0.3]\n",
    "records = []\n",
    "\n",
    "for stock, g in daily_embeddings.groupby(\"Stock_symbol\"):\n",
    "    g = g.sort_values(\"Date\")\n",
    "    E_arr = np.vstack(g[\"E_t\"].values).astype(np.float32)\n",
    "    \n",
    "    # Compute μ_{t−1} for all α\n",
    "    mu_dict = {}\n",
    "    for alpha in alphas:\n",
    "        mu_arr = compute_ema(E_arr, alpha)\n",
    "        mu_arr = np.vstack([mu_arr[0], mu_arr[:-1]])  # enforce t-1\n",
    "        mu_dict[alpha] = mu_arr\n",
    "    \n",
    "    # Compute SSD for all distances and α\n",
    "    for i, row in g.iterrows():\n",
    "        e = row[\"E_t\"]\n",
    "        for alpha, mu_arr in mu_dict.items():\n",
    "            mu = mu_arr[i - g.index[0]]\n",
    "            record = {\n",
    "                \"Stock_symbol\": row[\"Stock_symbol\"],\n",
    "                \"Date\": row[\"Date\"],\n",
    "                \"E_t\": e,\n",
    "                \"EMA_alpha\": alpha,\n",
    "                \"mu_t_minus_1\": mu,\n",
    "                \"SSD_cosine\": cosine_distance(e, mu),\n",
    "                \"SSD_l2\": l2_distance(e, mu),\n",
    "                \"SSD_angular\": angular_distance(e, mu)\n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "# Final DataFrame\n",
    "ssd_variants = pd.DataFrame(records, columns=[\n",
    "    \"Stock_symbol\", \"Date\", \"E_t\", \"EMA_alpha\", \"mu_t_minus_1\",\n",
    "    \"SSD_cosine\", \"SSD_l2\", \"SSD_angular\"\n",
    "])\n",
    "\n",
    "# Save final variants only\n",
    "ssd_variants.to_parquet(\"ssd_final_variants.parquet\", engine=\"pyarrow\", compression=\"zstd\")\n",
    "print(f\"✅ SSD variants computed: {ssd_variants.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e932bb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stock_symbol', 'Date', 'element', 'EMA_alpha', 'element', 'SSD_cosine', 'SSD_l2', 'SSD_angular']\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Open the Parquet file (does NOT load all data)\n",
    "table = pq.ParquetFile(\"ssd_final_variants.parquet\")\n",
    "\n",
    "# Get column names\n",
    "print(table.schema.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b6f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
